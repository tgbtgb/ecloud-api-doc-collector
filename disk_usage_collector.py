#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç£ç›˜ä½¿ç”¨é‡æ”¶é›†å™¨

è¯¥è„šæœ¬ç”¨äºæ”¶é›†æ‰€æœ‰æœåŠ¡å™¨çš„ç£ç›˜ä½¿ç”¨é‡ä¿¡æ¯ï¼Œå¹¶ç”ŸæˆExcelæŠ¥å‘Šã€‚
åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼šresourceNameã€åˆ†åŒºã€ç£ç›˜å®¹é‡å¤§å°ã€å·²ä½¿ç”¨å¤§å°ã€å·²ä½¿ç”¨ç™¾åˆ†æ¯”

ä½¿ç”¨æ–¹æ³•:
python disk_usage_collector.py --pool-id <èµ„æºæ± ID> [--product-type vm] [--output disk_usage_report.xlsx]
"""

import argparse
import requests
import pandas as pd
import json
import time
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional
import logging
from ecloud_auth import ECloudAuth

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DiskUsageCollector:
    """ç£ç›˜ä½¿ç”¨é‡æ”¶é›†å™¨ç±»"""
    
    def __init__(self, pool_id: str, access_key: str, secret_key: str, product_type: str = "vm", base_url: str = "https://api-wuxi-1.cmecloud.cn:8443"):
        self.pool_id = pool_id
        self.product_type = product_type
        self.access_key = access_key
        self.secret_key = secret_key
        self.base_url = base_url
        self.session = requests.Session()
        
        # åˆå§‹åŒ–é‰´æƒå®ä¾‹
        self.auth = ECloudAuth(access_key, secret_key)
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'Content-Type': 'application/json',
            'Accept': 'application/json',
            'Accept-Charset': 'utf-8',
            'Accept-Encoding': 'gzip, deflate',
            'User-Agent': 'DiskUsageCollector/1.0'
        })
        
        # ç£ç›˜ç›¸å…³æŒ‡æ ‡
        self.disk_metrics = {
            'vm_realtime_disk_total': 'å•ä¸ªåˆ†åŒºå®¹é‡',
            'vm_realtime_disk_used': 'å•ä¸ªåˆ†åŒºçš„ä½¿ç”¨é‡', 
            'vm_realtime_disk_percent': 'å•ä¸ªåˆ†åŒºä½¿ç”¨ç‡'
        }
    

    
    def get_resource_list(self, page_size: int = 100) -> List[Dict]:
        """è·å–èµ„æºåˆ—è¡¨"""
        logger.info(f"æ­£åœ¨è·å–èµ„æºåˆ—è¡¨ï¼Œèµ„æºæ± ID: {self.pool_id}ï¼Œäº§å“ç±»å‹: {self.product_type}")
        
        all_resources = []
        page_num = 1
        
        while True:
            servlet_path = "/api/edw/openapi/version2/v1/dawn/monitor/resources"
            params = {
                'poolId': self.pool_id,
                'productType': self.product_type,
                'pageNum': str(page_num),
                'pageSize': str(page_size)
            }
            
            try:
                # å¯¹è¯·æ±‚è¿›è¡Œç­¾å
                signed_params = self.auth.sign_request('GET', servlet_path, params)
                
                # æ‰‹åŠ¨æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œé¿å…requestså†æ¬¡ç¼–ç 
                query_string = self.auth.create_canonical_query_string(signed_params)
                
                # æ„å»ºå®Œæ•´URL
                url = f"{self.base_url}{servlet_path}?{query_string}"
                
                # æ·»åŠ å®Œæ•´çš„è¯·æ±‚å¤´
                host = self.base_url.replace('https://', '').replace('http://', '').split('/')[0]
                headers = {
                    'Host': host,
                    'Content-Type': 'application/json',
                    'Accept': 'application/json',
                    'Accept-Charset': 'utf-8'
                }
                
                logger.debug(f"è¯·æ±‚URL: {url}")
                logger.debug(f"è¯·æ±‚å¤´: {headers}")
                logger.debug(f"æŸ¥è¯¢å­—ç¬¦ä¸²: {query_string}")
                
                response = self.session.get(url, headers=headers)
                logger.debug(f"å“åº”çŠ¶æ€ç : {response.status_code}")
                logger.debug(f"å“åº”å†…å®¹: {response.text}")
                
                # å¦‚æœå“åº”ä¸æ˜¯200ï¼Œè®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                if response.status_code != 200:
                    logger.error(f"APIè¯·æ±‚å¤±è´¥ - çŠ¶æ€ç : {response.status_code}")
                    logger.error(f"å“åº”å¤´: {dict(response.headers)}")
                    logger.error(f"é”™è¯¯å“åº”å†…å®¹: {response.text}")
                    try:
                        error_data = response.json()
                        logger.error(f"é”™è¯¯è¯¦æƒ…: {error_data}")
                    except:
                        logger.error("æ— æ³•è§£æé”™è¯¯å“åº”ä¸ºJSON")
                
                response.raise_for_status()
                data = response.json()
                
                if data.get('code') != '000000':
                    logger.error(f"è·å–èµ„æºåˆ—è¡¨å¤±è´¥: {data.get('message')}")
                    break
                
                entity = data.get('entity', {})
                content = entity.get('content', [])
                
                if not content:
                    break
                    
                all_resources.extend(content)
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µ
                if page_num >= entity.get('pageCount', 1):
                    break
                    
                page_num += 1
                time.sleep(0.1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                logger.error(f"è·å–èµ„æºåˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                break
        
        logger.info(f"å…±è·å–åˆ° {len(all_resources)} ä¸ªèµ„æº")
        return all_resources
    
    def get_metric_indicators(self) -> List[Dict]:
        """è·å–äº§å“æ€§èƒ½æŒ‡æ ‡"""
        logger.info("æ­£åœ¨è·å–äº§å“æ€§èƒ½æŒ‡æ ‡")
        
        servlet_path = "/api/edw/openapi/version2/v1/dawn/monitor/distribute/metricindicators"
        params = {
            'poolId': self.pool_id,
            'productType': self.product_type
        }
        
        try:
            # å¯¹è¯·æ±‚è¿›è¡Œç­¾å
            signed_params = self.auth.sign_request('GET', servlet_path, params)
            
            # æ‰‹åŠ¨æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œé¿å…requestså†æ¬¡ç¼–ç 
            query_string = self.auth.create_canonical_query_string(signed_params)
            
            # æ„å»ºå®Œæ•´URL
            url = f"{self.base_url}{servlet_path}?{query_string}"
            
            # æ·»åŠ å®Œæ•´çš„è¯·æ±‚å¤´
            host = self.base_url.replace('https://', '').replace('http://', '').split('/')[0]
            headers = {
                'Host': host,
                'Content-Type': 'application/json',
                'Accept': 'application/json',
                'Accept-Charset': 'utf-8'
            }
            
            logger.debug(f"è¯·æ±‚URL: {url}")
            logger.debug(f"è¯·æ±‚å¤´: {headers}")
            logger.debug(f"æŸ¥è¯¢å­—ç¬¦ä¸²: {query_string}")
            
            response = self.session.get(url, headers=headers)
            logger.debug(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            logger.debug(f"å“åº”å†…å®¹: {response.text}")
            response.raise_for_status()
            data = response.json()
            
            if data.get('code') != '000000':
                logger.error(f"è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {data.get('message')}")
                return []
            
            indicators = data.get('entity', [])
            
            # ç­›é€‰ç£ç›˜ç›¸å…³æŒ‡æ ‡
            disk_indicators = []
            for indicator in indicators:
                if indicator.get('metricName') in self.disk_metrics:
                    disk_indicators.append(indicator)
            
            logger.info(f"æ‰¾åˆ° {len(disk_indicators)} ä¸ªç£ç›˜ç›¸å…³æŒ‡æ ‡")
            return disk_indicators
            
        except Exception as e:
            logger.error(f"è·å–æ€§èƒ½æŒ‡æ ‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return []
    
    def get_metric_nodes(self, resource_id: str, metric_name: str) -> List[str]:
        """è·å–æ€§èƒ½æŒ‡æ ‡å­èŠ‚ç‚¹åç§°ï¼ˆåˆ†åŒºä¿¡æ¯ï¼‰"""
        servlet_path = "/api/edw/openapi/version2/v1/dawn/monitor/distribute/metricnode"
        params = {
            'poolId': self.pool_id,
            'metricName': metric_name,
            'resourceId': resource_id
        }
        
        try:
            # å¯¹è¯·æ±‚è¿›è¡Œç­¾å
            signed_params = self.auth.sign_request('GET', servlet_path, params)
            
            # æ‰‹åŠ¨æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œé¿å…requestså†æ¬¡ç¼–ç 
            query_string = self.auth.create_canonical_query_string(signed_params)
            
            # æ„å»ºå®Œæ•´URL
            url = f"{self.base_url}{servlet_path}?{query_string}"
            
            # æ·»åŠ å®Œæ•´çš„è¯·æ±‚å¤´
            host = self.base_url.replace('https://', '').replace('http://', '').split('/')[0]
            headers = {
                'Host': host,
                'Content-Type': 'application/json',
                'Accept': 'application/json',
                'Accept-Charset': 'utf-8'
            }
            
            logger.debug(f"è¯·æ±‚URL: {url}")
            logger.debug(f"è¯·æ±‚å¤´: {headers}")
            logger.debug(f"æŸ¥è¯¢å­—ç¬¦ä¸²: {query_string}")
            
            response = self.session.get(url, headers=headers)
            logger.debug(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            logger.debug(f"å“åº”å†…å®¹: {response.text}")
            response.raise_for_status()
            data = response.json()
            
            if data.get('code') != '000000':
                logger.warning(f"è·å–èµ„æº {resource_id} çš„ {metric_name} å­èŠ‚ç‚¹å¤±è´¥: {data.get('message')}")
                return []
            
            return data.get('entity', [])
            
        except Exception as e:
            logger.warning(f"è·å–èµ„æº {resource_id} çš„ {metric_name} å­èŠ‚ç‚¹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return []
    
    def get_performance_data(self, resource_id: str, metrics: List[Dict], 
                           start_time: str, end_time: str) -> List[Dict]:
        """è·å–æ€§èƒ½æ•°æ®"""
        servlet_path = "/api/edw/openapi/version2/v1/dawn/monitor/distribute/fetch"
        params = {'poolId': self.pool_id}
        
        payload = {
            'startTime': start_time,
            'endTime': end_time,
            'resourceId': resource_id,
            'productType': self.product_type,
            'performanceDataAggType': 'MAX',
            'metrics': metrics
        }
        
        try:
            # å¯¹è¯·æ±‚è¿›è¡Œç­¾å
            signed_params = self.auth.sign_request('POST', servlet_path, params)
            
            # æ‰‹åŠ¨æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œé¿å…requestså†æ¬¡ç¼–ç 
            query_string = self.auth.create_canonical_query_string(signed_params)
            
            # æ„å»ºå®Œæ•´URL
            url = f"{self.base_url}{servlet_path}?{query_string}"
            
            # æ·»åŠ å®Œæ•´çš„è¯·æ±‚å¤´
            host = self.base_url.replace('https://', '').replace('http://', '').split('/')[0]
            headers = {
                'Host': host,
                'Content-Type': 'application/json',
                'Accept': 'application/json',
                'Accept-Charset': 'utf-8'
            }
            
            logger.debug(f"è¯·æ±‚URL: {url}")
            logger.debug(f"è¯·æ±‚å¤´: {headers}")
            logger.debug(f"æŸ¥è¯¢å­—ç¬¦ä¸²: {query_string}")
            logger.debug(f"è¯·æ±‚ä½“: {payload}")
            
            response = self.session.post(url, json=payload, headers=headers)
            logger.debug(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            logger.debug(f"å“åº”å†…å®¹: {response.text}")
            response.raise_for_status()
            data = response.json()
            
            if data.get('code') != '000000':
                logger.warning(f"è·å–èµ„æº {resource_id} æ€§èƒ½æ•°æ®å¤±è´¥: {data.get('message')}")
                return []
            
            return data.get('entity', [])
            
        except Exception as e:
            logger.warning(f"è·å–èµ„æº {resource_id} æ€§èƒ½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return []
    
    def collect_disk_usage_data(self, start_time: str = None, end_time: str = None) -> List[Dict]:
        """æ”¶é›†ç£ç›˜ä½¿ç”¨é‡æ•°æ®
        
        Args:
            start_time: å¼€å§‹æ—¶é—´ï¼Œæ ¼å¼ï¼š'YYYY-MM-DD HH:MM:SS'ï¼Œå¦‚æœæœªæä¾›åˆ™é»˜è®¤ä¸ºå½“å‰æ—¶é—´-1å¤©
            end_time: ç»“æŸæ—¶é—´ï¼Œæ ¼å¼ï¼š'YYYY-MM-DD HH:MM:SS'ï¼Œå¦‚æœæœªæä¾›åˆ™é»˜è®¤ä¸ºå½“å‰æ—¶é—´
        """
        logger.info("å¼€å§‹æ”¶é›†ç£ç›˜ä½¿ç”¨é‡æ•°æ®")
        
        # è·å–èµ„æºåˆ—è¡¨
        resources = self.get_resource_list()
        if not resources:
            logger.error("æœªè·å–åˆ°ä»»ä½•èµ„æº")
            return []
        
        # è·å–ç£ç›˜ç›¸å…³æŒ‡æ ‡
        indicators = self.get_metric_indicators()
        if not indicators:
            logger.error("æœªè·å–åˆ°ç£ç›˜ç›¸å…³æŒ‡æ ‡")
            return []
        
        # è®¾ç½®æŸ¥è¯¢æ—¶é—´èŒƒå›´
        if not start_time or not end_time:
            # å¦‚æœæ²¡æœ‰ä¼ å…¥æ—¶é—´ï¼Œé»˜è®¤æŸ¥è¯¢è¿‡å»24å°æ—¶çš„æ•°æ®
            end_time_dt = datetime.now()
            start_time_dt = end_time_dt - timedelta(days=1)
            start_time_str = start_time_dt.strftime('%Y-%m-%d %H:%M:%S')
            end_time_str = end_time_dt.strftime('%Y-%m-%d %H:%M:%S')
            logger.info(f"ä½¿ç”¨é»˜è®¤æ—¶é—´èŒƒå›´ï¼š{start_time_str} åˆ° {end_time_str}")
        else:
            start_time_str = start_time
            end_time_str = end_time
            logger.info(f"ä½¿ç”¨æŒ‡å®šæ—¶é—´èŒƒå›´ï¼š{start_time_str} åˆ° {end_time_str}")
        
        all_disk_data = []
        
        for i, resource in enumerate(resources, 1):
            resource_id = resource.get('resourceId')
            resource_name = resource.get('resourceName')
            
            logger.info(f"æ­£åœ¨å¤„ç†èµ„æº {i}/{len(resources)}: {resource_name} ({resource_id})")
            
            # è·å–è¯¥èµ„æºçš„åˆ†åŒºä¿¡æ¯ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªç£ç›˜æŒ‡æ ‡è·å–åˆ†åŒºåˆ—è¡¨ï¼‰
            first_metric = next(iter(self.disk_metrics.keys()))
            partitions = self.get_metric_nodes(resource_id, first_metric)
            
            if not partitions:
                logger.warning(f"èµ„æº {resource_name} æœªæ‰¾åˆ°åˆ†åŒºä¿¡æ¯ï¼Œå°†åˆ›å»ºé»˜è®¤è®°å½•")
                # å³ä½¿æ²¡æœ‰åˆ†åŒºä¿¡æ¯ï¼Œä¹Ÿè¦åˆ›å»ºä¸€æ¡è®°å½•
                partition_data = {
                    'resourceName': resource_name,
                    'resourceId': resource_id,
                    'partition': 'N/A',
                    'disk_total': 0,
                    'disk_used': 0,
                    'disk_percent': 0
                }
                all_disk_data.append(partition_data)
                continue
            
            logger.info(f"èµ„æº {resource_name} æ‰¾åˆ° {len(partitions)} ä¸ªåˆ†åŒº: {partitions}")
            
            # ä¸ºæ¯ä¸ªåˆ†åŒºæ”¶é›†æ•°æ®
            for partition in partitions:
                partition_data = {
                    'resourceName': resource_name,
                    'resourceId': resource_id,
                    'partition': partition,
                    'disk_total': 0,  # é»˜è®¤å€¼è®¾ä¸º0
                    'disk_used': 0,   # é»˜è®¤å€¼è®¾ä¸º0
                    'disk_percent': 0 # é»˜è®¤å€¼è®¾ä¸º0
                }
                
                try:
                    # æ„å»ºæŸ¥è¯¢æŒ‡æ ‡
                    metrics_to_query = []
                    for metric_name in self.disk_metrics.keys():
                        metrics_to_query.append({
                            'metricName': metric_name,
                            'metricNodeName': partition
                        })
                    
                    # è·å–æ€§èƒ½æ•°æ®
                    performance_data = self.get_performance_data(
                        resource_id, metrics_to_query, start_time_str, end_time_str
                    )
                    
                    # è§£ææ€§èƒ½æ•°æ®
                    for perf_item in performance_data:
                        metric_name = perf_item.get('metricName')
                        avg_value = perf_item.get('avgValue')
                        
                        if avg_value is not None:
                            if metric_name == 'vm_realtime_disk_total':
                                partition_data['disk_total'] = round(avg_value, 2)
                            elif metric_name == 'vm_realtime_disk_used':
                                partition_data['disk_used'] = round(avg_value, 2)
                            elif metric_name == 'vm_realtime_disk_percent':
                                partition_data['disk_percent'] = round(avg_value, 2)
                    
                    # å¦‚æœè·å–åˆ°çš„æ€§èƒ½æ•°æ®ä¸ºç©ºï¼Œè®°å½•è­¦å‘Šä½†ä¿æŒé»˜è®¤å€¼0
                    if not performance_data:
                        logger.warning(f"èµ„æº {resource_name} åˆ†åŒº {partition} æœªè·å–åˆ°æ€§èƒ½æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼0")
                        
                except Exception as e:
                    logger.warning(f"è·å–èµ„æº {resource_name} åˆ†åŒº {partition} æ€§èƒ½æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤å€¼0")
                
                all_disk_data.append(partition_data)
                time.sleep(0.1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        logger.info(f"æ•°æ®æ”¶é›†å®Œæˆï¼Œå…±æ”¶é›†åˆ° {len(all_disk_data)} æ¡ç£ç›˜ä½¿ç”¨é‡è®°å½•")
        return all_disk_data
    
    def export_to_excel(self, data: List[Dict], output_file: str):
        """å¯¼å‡ºæ•°æ®åˆ°Excelæ–‡ä»¶"""
        logger.info(f"æ­£åœ¨å¯¼å‡ºæ•°æ®åˆ°Excelæ–‡ä»¶: {output_file}")
        
        if not data:
            logger.warning("æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")
            return
        
        # åˆ›å»ºDataFrame
        df_data = []
        for item in data:
            df_data.append({
                'èµ„æºåç§°': item['resourceName'],
                'èµ„æºID': item['resourceId'],
                'åˆ†åŒº': item['partition'],
                'ç£ç›˜å®¹é‡å¤§å°(GB)': item['disk_total'] if item['disk_total'] is not None else 0,
                'å·²ä½¿ç”¨å¤§å°(GB)': item['disk_used'] if item['disk_used'] is not None else 0,
                'å·²ä½¿ç”¨ç™¾åˆ†æ¯”(%)': item['disk_percent'] if item['disk_percent'] is not None else 0
            })
        
        df = pd.DataFrame(df_data)
        
        # å¯¼å‡ºåˆ°Excel
        try:
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='ç£ç›˜ä½¿ç”¨é‡æŠ¥å‘Š', index=False)
                
                # è·å–å·¥ä½œè¡¨å¹¶è°ƒæ•´åˆ—å®½
                worksheet = writer.sheets['ç£ç›˜ä½¿ç”¨é‡æŠ¥å‘Š']
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
            
            logger.info(f"Excelæ–‡ä»¶å¯¼å‡ºæˆåŠŸ: {output_file}")
            logger.info(f"å…±å¯¼å‡º {len(df_data)} æ¡è®°å½•")
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºExcelæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç£ç›˜ä½¿ç”¨é‡æ”¶é›†å™¨')
    parser.add_argument('--pool-id', required=True, help='èµ„æºæ± ID')
    parser.add_argument('--product-type', default='vm', help='äº§å“ç±»å‹ï¼ˆé»˜è®¤: vmï¼‰')
    parser.add_argument('--output', default='disk_usage_report.xlsx', help='è¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤: disk_usage_report.xlsxï¼‰')
    parser.add_argument('--access-key', required=True, help='ç§»åŠ¨äº‘Access Key ID (AK)')
    parser.add_argument('--secret-key', required=True, help='ç§»åŠ¨äº‘Secret Access Key (SK)')
    parser.add_argument('--base-url', default='https://api-wuxi-1.cmecloud.cn:8443', 
                       help='APIåŸºç¡€URLï¼ˆé»˜è®¤: https://api-wuxi-1.cmecloud.cn:8443ï¼‰')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--start-time', help='å¼€å§‹æ—¶é—´ï¼Œæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºå½“å‰æ—¶é—´-1å¤©ï¼‰')
    parser.add_argument('--end-time', help='ç»“æŸæ—¶é—´ï¼Œæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SSï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºå½“å‰æ—¶é—´ï¼‰')
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºè¯¦ç»†æ—¥å¿—')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.info("å·²å¯ç”¨è°ƒè¯•æ¨¡å¼")
    
    # å¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼Œå°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°
    access_key = args.access_key
    secret_key = args.secret_key
    base_url = args.base_url
    
    if args.config:
        try:
            import importlib.util
            spec = importlib.util.spec_from_file_location("config", args.config)
            config = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(config)
            
            access_key = getattr(config, 'ACCESS_KEY', access_key)
            secret_key = getattr(config, 'SECRET_KEY', secret_key)
            base_url = getattr(config, 'BASE_URL', base_url)
            
            logger.info(f"å·²ä»é…ç½®æ–‡ä»¶åŠ è½½å‚æ•°: {args.config}")
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {args.config}: {str(e)}ï¼Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°")
    
    # éªŒè¯å¿…è¦å‚æ•°
    if not access_key or not secret_key:
        logger.error("å¿…é¡»æä¾›Access Keyå’ŒSecret Key")
        parser.print_help()
        return
    
    # åˆ›å»ºæ”¶é›†å™¨å®ä¾‹
    collector = DiskUsageCollector(
        pool_id=args.pool_id,
        product_type=args.product_type,
        base_url=base_url,
        access_key=access_key,
        secret_key=secret_key
    )
    
    try:
        logger.info(f"å¼€å§‹æ”¶é›†ç£ç›˜ä½¿ç”¨é‡æ•°æ®...")
        logger.info(f"èµ„æºæ± ID: {args.pool_id}")
        logger.info(f"äº§å“ç±»å‹: {args.product_type}")
        logger.info(f"APIåœ°å€: {base_url}")
        
        # æ”¶é›†æ•°æ®
        disk_data = collector.collect_disk_usage_data(
            start_time=args.start_time,
            end_time=args.end_time
        )
        
        if not disk_data:
            logger.warning("æœªæ”¶é›†åˆ°ä»»ä½•ç£ç›˜ä½¿ç”¨é‡æ•°æ®")
            return
        
        # å¯¼å‡ºåˆ°Excel
        collector.export_to_excel(disk_data, args.output)
        
        print(f"\nâœ… ç£ç›˜ä½¿ç”¨é‡æŠ¥å‘Šå·²ç”Ÿæˆ: {args.output}")
        print(f"ğŸ“Š å…±æ”¶é›†åˆ° {len(disk_data)} æ¡è®°å½•")
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise

if __name__ == '__main__':
    main()